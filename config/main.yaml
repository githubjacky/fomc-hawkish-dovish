defaults:
  - _self_


# options: rnn, sbert, cls, cls_pooler, last_layer_mean, last_layer_mean_pooler
# pooler means to train the BERT pooling layer
pooling_strategy: cls_pooler

flair_embed:
  model_name: bert-base-uncased
  flair_layers: "all"
  flair_layer_mean: true

sbert_embed:
  model_name: sentence-transformers/all-mpnet-base-v2 

nn: GRU
# should adjust only in sentence-level classification
# for word-level, flair's word embeddings is the only choice
lr: 0.000005124067949679128
batch_size: 712
random_state: 1126

early_stop:
  monitor: val/macro_avg_prec
  mode: max
  patience: 5  # log every `patience * check_val_every_n_epoch` epochs

model_check_point:
  monitor: val/macro_avg_prec
  mode: max

trainer:
  accelerator: gpu
  strategy: auto
  devices:
    - 0
  num_nodes: 1
  precision: 32-true
  max_epochs: 300
  log_every_n_steps: 1
  check_val_every_n_epoch: 5
  # accumulate_grad_batches:
  # gradient_clip_val:

tuning:
  study_name: cls_pooling_cls_pooler
  n_trials: 75

RNNFamily:
  hidden_size: 1630
  num_layers: 1
  dropout: 0
  bidirectional: True

MLP:
  hidden_size: 256
  n_layers: 1
  dropout: 0.2
  output_size: 128

ff:
  ff_dropout: 0.17831726122503855
