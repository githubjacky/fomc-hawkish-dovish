defaults:
  - _self_


# pooler means to train the BERT pooling layer
# options: rnn, sbert, cls, cls_pooler, cls_pooler_output, last_layer_mean, last_layer_mean_pooler
# (fine-tune) options: finetune_pooler_output, finetune_cls finetune_last_layer_mean finetune_last_layer_mean_pooler
pooling_strategy: finetune_pooler_output

lr: 0.05367370559450229
batch_size: 512

tuning:
  study_name: finetune_pooler_output_twitter-roberta-base-sentiment-latest_v3
  n_trials: 300
  
flair_embed:
  model_name: cardiffnlp/twitter-roberta-base-sentiment-latest
  flair_layers: "all"
  flair_layer_mean: true

sbert_embed:
  model_name: FinLang/finance-embeddings-investopedia

nn: LSTM
# should adjust only in sentence-level classification
# for word-level, flair's word embeddings is the only choice
random_state: 1126

early_stop:
  monitor: val/macro_avg_prec
  mode: max
  patience: 20  # log every `patience * check_val_every_n_epoch` epochs

model_check_point:
  monitor: val/macro_avg_prec
  mode: max

trainer:
  accelerator: gpu
  strategy: auto
  devices:
    - 0
  num_nodes: 1
  precision: 32-true
  max_epochs: 5000
  log_every_n_steps: 1
  check_val_every_n_epoch: 5
  # accumulate_grad_batches:
  # gradient_clip_val:


RNNFamily:
  hidden_size: 1072
  num_layers: 1
  dropout: 0
  bidirectional: True

MLP:
  hidden_size: 256
  n_layers: 1
  dropout: 0.2
  output_size: 128

ff:
  ff_input_size: 1378
  ff_dropout: 0.22998956431876594
