defaults:
  - _self_


# options: rnn, sbert, cls, cls_pooler, cls_pooler_output, last_layer_mean, last_layer_mean_pooler
# pooler means to train the BERT pooling layer
pooling_strategy: cls

lr: 0.0011362651459171757
batch_size: 265

tuning:
  study_name: cls_distilroberta-finetuned-financial-text-classification
  n_trials: 300
  
flair_embed:
  model_name: nickmuchi/distilroberta-finetuned-financial-text-classification
  flair_layers: "all"
  flair_layer_mean: true

sbert_embed:
  model_name: sentence-transformers/multi-qa-mpnet-base-dot-v1

nn: LSTM
# should adjust only in sentence-level classification
# for word-level, flair's word embeddings is the only choice
random_state: 1126

early_stop:
  monitor: val/macro_avg_prec
  mode: max
  patience: 5  # log every `patience * check_val_every_n_epoch` epochs

model_check_point:
  monitor: val/macro_avg_prec
  mode: max

trainer:
  accelerator: gpu
  strategy: auto
  devices:
    - 0
  num_nodes: 1
  precision: 32-true
  max_epochs: 5000
  log_every_n_steps: 1
  check_val_every_n_epoch: 5
  # accumulate_grad_batches:
  # gradient_clip_val:


RNNFamily:
  hidden_size: 1072
  num_layers: 1
  dropout: 0
  bidirectional: True

MLP:
  hidden_size: 256
  n_layers: 1
  dropout: 0.2
  output_size: 128

ff:
  ff_input_size: 256
  ff_dropout: 0.21807975814570435
